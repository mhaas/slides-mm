\documentclass[12pt,a4paper]{beamer}
\usepackage[utf8x]{inputenc}
\usepackage{ucs}
%\usepackage[english]{babel}
\usepackage[german]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\author{Michael Haas, haas@cl.uni-heidelberg.de}
\title{Johns \& Jones: Construction in Semantic Memory: Generating Perceptual Representations with Semantic Similarity}
\subtitle{Seminar: Multimodale Semantik (Dr. ric. Gerhard Kremer)}
\date{21-05-2013}
\begin{document}


\begin{frame}
\maketitle
\end{frame}

\begin{frame}{Overview}
\begin{itemize}
\item Idea \& Approach %- Dist. Semantics, PSS, Perceptual Simulation
\item Data
%\item Background PSS
\item Evaluation
    \item Model Foundations
    \item Behavioral Simulations
\item Recap \& Discussion
\item \textbf{Questions? Ask!}
\end{itemize}
\end{frame}


\begin{frame}{Idea \& Approach}
\begin{itemize}
\item Given
    \begin{itemize}
    \item Linguistic representation for \textbf{all} words
    \item Perceptual representation for \textbf{some} words
    \end{itemize}
\item Learn
    \begin{itemize}
    \item perceptual representations for \textbf{all} words
    \item based on Perceptual Systems Theory
    \item integrated model of linguistic and perceptual information
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Data: Linguistic data set}
\begin{itemize}
\item Words represented as binary co-occurrence vectors
\item Co-occurence counted across documents
\item Extraxted from 250,000 Wikipedia articles
\end{itemize}
\end{frame}


\begin{frame}{Data: Perceptual data set}
\begin{itemize}
\item Probability vectors over perceptual features
\item Generated by \textbf{humans}
\item May include:
    \begin{itemize}
    \item taxonomic features
    \item situational features
    \end{itemize}
\item Different data sources:
    \begin{itemize}
    \item feature generation norms
    \item modality exclusivity norms
    \end{itemize}

\end{itemize}
\end{frame}

\begin{frame}{Data: Full lexical representation}
\begin{itemize}
\item Concatenation of linguistic + perceptual data set
\end{itemize}
\end{frame}


\begin{frame}{Approach: Inference}
% use \text{} to escape Perc?
$$ Perc_{j} = \sum_{i=1}^{M}{T_{i} \times S(T_{i}, T_{j})^{\lambda} } $$
% M: size of lexicon, T lexical trace, S similarity function (cosine), $\lambda$ similarity weighting parameter
\begin{itemize}
\item Step 1: Only Calculate $Perc$ for words with zero perceptual vector
\item Step 2: (Re)-Calculate $Perc$ for all words
    \begin{itemize}
    \item perceptual features are inferred via global similarity to all lexical entries
    \item $\to$ aggregation of linguistic and perceptual information from step 1
    \end{itemize}
\item $\to$ Make predictions about perceptual properties/features of words (from global similarity structure)
\end{itemize}
\end{frame}




\begin{frame}{Evaluation}
\begin{itemize}
    \item Model Foundations
        \begin{itemize}
        \item $\to$ How do model parameters affect performance?
        \end{itemize}
    \item Behavioral Simulations
        \begin{itemize}
        \item $\to$ Does the approach model behavioral phenomena from grounded cognition?
        \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Evaluation: Foundations: Word norms}
\begin{itemize}
\item Cross-Validation procedure: infer perceptual representation
\item Measure: correlation between inferred and actual perceptual representation
% TODO: What actual correlation measure is used?
\end{itemize}
% Model predictions for each Word Norm
\begin{table}
    \begin{tabular}{lll}
    Word Norm          & Step 1 & Step 2 \\
    McRae, et al.      & 0.42   & 0.72   \\
    Vinson \& Vigglioco & 0.42   & 0.77   \\
    Lynott \& Connell   & 0.83   & 0.85   \\
    \end{tabular}
\end{table}
\end{frame}

\begin{frame}{Evaluation: Foundations: Lexicon Size}
\begin{itemize}
\item Effect of lexicon size on inferred perceptual representation
\item Use $2000 \to 24000$ most frequent words
\item Result: noise accumulation reduces model performance
% TODO: frequency distribution not provided, compare Zipf?
\end{itemize}
\includegraphics[scale=0.8]{figure_2_effect_of_lexicon_size.png}

\end{frame}


\begin{frame}{Evaluation: Foundations: Effect of Corpus Size}
\begin{itemize}
\item Model trained on
    \begin{itemize}
    \item Wikipedia Corpus, n=250,000: $r = 0.64$
    \item TASA corpus, n=37,600: $r = 0.77$
    \end{itemize}
\item Result: increasing number and diversity of contexts improves performance
% "the greater the amount of experience the model has with language, the better its inferences
%are about a word's perceptual representation"%
\end{itemize}
\end{frame}

\begin{frame}{Evaluation: Foundations: Reverse Inference}
\begin{itemize}
\item Infer linguistic distribution from perceptual information
    \begin{itemize}
    \item Infer ''forward'' perceptual data from linguistic data
    \item Infer reverse linguistic data from learned perceptual data
    \end{itemize}
\item Measure: correlation between inferred co-occurence vector and original
\item For nouns by McRae, et al $r = 0.67$
\item Rest $r = 0.5$
%Reasons: 1) perceptual space does not extend to all words, 2) abstract words%
\item Result: Lexical inferences possible in both directions
\end{itemize}
\end{frame}


\begin{frame}{Evaluation: Behavioral: Affordance}
\begin{itemize}
\item ''Hang the coat on the --------''
    \begin{itemize}
    \item ''coat rack'' - \textbf{realistic}
    \item ''vacuum cleaner'' - \textbf{afforded}
    \item ''cup'' - \textbf{non-afforded}
    \end{itemize}
\item Experiment: calculate similarity between central action word and objects for
    \begin{itemize}
    \item inferred perceptual vectors
    \item raw co-occurrence vectors
    \end{itemize}
% Only perceptual feature vectors or complete lexical entry? %
\item Result: inferred perceptual vectors provide better distinction between affordance ratings
% TODO: Show which ones are statistically significant in figure 3 % 
\end{itemize}

\end{frame}

\begin{frame}{Evaluation: Behavioral: Sensory/motor based priming}
% Neural facilitation in neuroscience, is the increase in postsynaptic potential evoked by a 2nd impulse. %
\begin{itemize}
\item Priming: ''piano'' $\to$ ''typewriter''
\item Common manipulation aspect
\item Experiment: comparison of similarity between related-target and unrelated-target
\item Measure: magnitude of facilitation
    \begin{itemize}
    \item difference not significant for raw co-occurrence representations
    \item difference significant for inferred perceptual representations
    \end{itemize}
\item Result: model successfully infers perceptual information
\end{itemize}
\end{frame}


\begin{frame}{Evaluation: Behaviorial: Phrase/referent similarity}
\begin{itemize}
\item Data set: novel and familiar noun phrases 
\item Data set: familiarity rated by test subjects
    \begin{itemize}
    \item ''smashed tomato'' VS ''sliced tomato''
    \end{itemize}
\item Measure: ''familarity'' computed as cosine between two words
\item Marginally significant difference between novel and familiar condition
\item But: significant correlation between computed familiarity and subject ratings % on item level
    \begin{itemize}
    \item For inferred perceptual representations only!
    \end{itemize}
\item Result: inferred perceptual representation can simulate phrase and referent similarity
\end{itemize}
\end{frame}


\begin{frame}{Evaluation: Behavioral: Inferred Modality Representation}
\begin{itemize}
\item Words represented as probability distribution over five modalities
\item Modalities: visual, auditory, tactile, olfactory, gustatory
\item Experiment:
    \begin{itemize}
    \item in model, measure strength for known dominant modality
    \item compare against random sample from different modality
    \end{itemize}
\item Result: Correct inferences about modality basis of words
% TODO: does this experiment really show that the correct basis is predicted?
% E.g. olfactory: other modality could still be stronger
\end{itemize}
\end{frame}


\begin{frame}{Recap}
\begin{itemize}
\item Simple model integrating linguistic and perceptual data
\item Allows inference of perceptual vectors from incomplete data via global lexical similarity
\item Based on Perceptual Systems Theory
\item Allows integration of various perceptual data sources
\item Thoughts:
    \begin{itemize}
    \item Joint learning instead of vector concatenation?
    \item Acquisition bottleneck for perceptual data
    \end{itemize}

\end{itemize}
\end{frame}




\begin{frame}{References}
\begin{thebibliography}{-}
% APA
\bibitem{jj} Johns, B. T., \& Jones, M. N. (2011). Construction in semantic memory: Generating perceptual representations with global lexical similarity. In Expanding the space of cognitive science: Proceedings of the 33rd Annual Meeting of the Cognitive Science Society (pp. 767-772).
\end{thebibliography}
\end{frame}
\end{document}
